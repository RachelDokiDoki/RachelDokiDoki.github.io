---
title: Qwen3 Moe Router分析
date: 2025-07-25 17:00:00 +0800
author: Aatrox
tags: [LLM]
math: true
---

# Qwen3MoeSparseMoeBlock forward (Moe Router)分析
self.top_k=8  
self.num_experts=128  
self.hidden_size = 4096  
sequence length = 4096  
```python
routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
```
selected_experts的shape为`(4096, 8)`，表示每个token选择的8个expert。  
例如`selected_experts[2][3]=78`，表示第2个token的第3个专家选择了78号expert  

```python
expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)
```
one_hot操作后未permute前的shape为`(4096, 8, 128)`，表示每个 token 选择了 top-8 个expert，每个expert用长度 128 的 one-hot 向量表示。  
例如`one_hot[2][3][70]=1`，表示第2个token选择的第3个expert是70号expert。  

permute后的`expert_mask的shape`为`(128, 8, 4096)`，表示该每个expert被哪些token作为第几个专家选中了。  
例如`expert_mask[5][3][300]=1`，表示5号expert，作为token 300的第3个专家被选中。  

```python
idx, top_x = torch.where(expert_mask[expert_idx])
```
这段代码可以解释为:
对于`expert_idx`号专家，作为被token `top_x[i]`的第`idx[i]`号expert选中。  

```python
current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)
```
`current_state`为当前token对应的hidden_states，shape为`(N_selected, hidden_dim)`  

```python
current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]
```
`routing_weights`的shape为`(4096, 8)`  
`routing_weights[top_x, idx, None]`的shape为`(N_selected, 1)`  
`expert_layer`是一个小的 MLP 层，对应地输入和输出都是 `(N_selected, hidden_dim)`  
`current_hidden_states`的shape为`(N_selected, hidden_dim)`  

```python
final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
```
`top_x`为当前选择token的索引，每个专家计算完后，需要将对应结果加回到主输出`final_hidden_states`中。
